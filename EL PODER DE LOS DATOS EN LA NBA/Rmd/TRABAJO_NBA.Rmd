---
title: "EL PODER DE LOS DATOS EN LA NBA"
author: "Mateo Alís, Sergio Ortiz, Manuel Caballero, Joel Porcar, Maria Porta, Xavier Ventura"
grupo: "A2-26"
output:
  word_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: sentence
---

El objetivo de este trabajo es ver que estadísticas de jugadores de la NBA nos permite condensar decenas de métricas (puntos, rebotes, asistencias, etc.) en unas pocas componentes que explican la mayor parte de la variabilidad, facilitando la visualización de perfiles de juego, la detección de jugadores atípicos y la segmentación en tipos (tiradores, interiores, organizadores).
Además de realizar un clustering de gran importancia para los GM de una franquicia que desee fichar a un jugador en mente, pero bien sea o porque no se pueden hacer cargo del contrato del jugador, o bien porque no se puede realizar ese traspaso, buscar alternativas para obtener el jugador más parecido que haya.
Y por último, queríamos realizar un modelo de predicción utilizando el PLS-DA para predecir aquellos jugadores que pueden ser all-star en la liga ya que todo jugador que está en ella alguna vez en su vida a soñado con serlo.

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo    = FALSE,    # no mostrar código
  warning = FALSE,    # no mostrar avisos
  message = FALSE,     # no mostrar mensajes
  results = 'hide'
)
```

```{r Librerias, include = FALSE}
library(factoextra)
library(FactoMineR)
library(readr)
library(corrplot)
library(dplyr)
library(knitr)
library(gridExtra)
library(tidyr)
library(GGally)
library(knitr)
library(cluster)
library(FactoMineR)
library(factoextra)
library(NbClust)
library(clValid)
library(tidyverse)
# Paquete ropls para PLS/PLS-DA
library(ropls)
# Librerías para visualizaciones
library(viridis)    # para paleta de colores
library(patchwork)  # para combinar ggplots
library(dplyr)      # para manipulación de tablas
library(ggplot2)    # para gráficos con grammar of graphics
library(ggrepel)    # para etiquetas en ggplot2

```

# Limpieza y Tratamiento de los datos

Para comenzar con el estudio de los jugadores de la NBA, primero cargamos los datos desde un archivo CSV con 679 filas.
Este archivo contiene información sobre las estadísticas de los jugadores durante la temporada 2022-2023.
Para facilitar el estudio de las variables, se clasificarán automáticamente como numéricas o categóricas.

```{r Lectura y preparación de datos}
# 1. Cargar el dataset
nba <- read.csv("nba_2022_2023.csv", as.is = TRUE)


# 2. Clasificar automáticamente cada columna como numérica o categórica
tipos <- sapply(nba, class)

# 3. Crear un dataframe para describir las variables
tipos_simple <- ifelse(
  tipos %in% c("numeric", "integer") & sapply(nba, function(x) length(unique(x)) > 2),
  "numerical",
  "categorical"
)
descNba <- data.frame(
  variable = names(tipos),
  tipo = tipos_simple,
  stringsAsFactors = FALSE
)
rownames(descNba) <- descNba$variable
descNba

```

```{r Revisión nulos o duplicados}

# 1. Revisar si hay valores nulos
sum(is.na(nba))

# 2. Revisar si hay duplicados
sum(duplicated(nba))

# 3. Revisar si hay duplicados en la columna "Jugador"
duplicados_jugador = nba[duplicated(nba$Jugador), "Jugador"]
length(duplicados_jugador)
```

Se observa que no tenemos valores nulos ni filas duplicadas en el dataset, pero se observa que aparecen 140 jugadores duplicados, lo que indica que hay jugadores que han jugado en más de un equipo durante la temporada 2022-2023.
Esto es normal en la NBA, ya que los jugadores pueden ser traspasados entre equipos.
Se observa que los jugadores que aparecen varias veces tienen en la columna Equipo el valor "TOT", siendo el total de las estadísticas de ese jugador en todos los equipos en los que ha jugado (Ver rmarkdown en *ANEXO 1*.)

```{r Escalado de datos }
nba_unique <- nba %>% 
  group_by(jugador) %>%                 # agrupa por nombre de jugador
  slice(if (n() == 1) 1                 # si solo hay una fila, se queda
        else which(equipo == "TOT")[1] # si hay varias, elige la fila con equipos == "TOT"
  ) %>% 
  ungroup()

length(unique(nba_unique$Jugador)) # número de jugadores únicos
```

Por regla general cuanto más juega un jugador, más puntos debería anotar.
Sin embargo, esto no siempre es cierto: hay jugadores que, aunque disputen muchos minutos, tienen un rol más pasador o defensivo, y otros que, jugando poco, son extremadamente eficientes ofensivamente.
Por lo tanto, es importante tener en cuenta el número de minutos jugados y los puntos anotados por partido para evaluar el rendimiento de un jugador.
Calculamos las estadísticas por minuto real, y ponderaremos por minutos jugados por partido con la función log(x+1) de manera que demos más peso a los que más minutos juegan sin exagerarlo.
Con esto conseguimos que los jugadores que son muy productivos pero apenas juegan ya no se inflan y será una mejor escala para el análisis.

```{r Escalado de datos reales }

# Creamos una nueva versión del dataset con variables ajustadas


nba_ajustado <- nba_unique %>%
  mutate(
    # Tiros de campo
    tiros_anotados_aj = (tiros_anotados_pp / minutos_por_partido) * log1p(minutos_por_partido),
    tiros_intentados_aj = (tiros_intentados_pp / minutos_por_partido) * log1p(minutos_por_partido),
    
    # Triples
    triples_anotados_aj = (triples_anotados_pp / minutos_por_partido) * log1p(minutos_por_partido),
    tripes_intentados_aj = (tripes_intentados_pp / minutos_por_partido) * log1p(minutos_por_partido),
    
    # Tiros de 2 puntos
    tiros_2_anotados_aj = (tiros_de_2_anotados_pp / minutos_por_partido) * log1p(minutos_por_partido),
    tiros_2_intentados_aj = (tiros_de_2_intentados_pp / minutos_por_partido) * log1p(minutos_por_partido),
    
    # Tiros libres
    tl_anotados_aj = (tl_anotados_pp / minutos_por_partido) * log1p(minutos_por_partido),
    tl_intentados_aj = (tl_intentados_pp / minutos_por_partido) * log1p(minutos_por_partido),
    
    # Rebotes
    ro_aj = (ro_pp / minutos_por_partido) * log1p(minutos_por_partido),
    rd_aj = (rd_pp / minutos_por_partido) * log1p(minutos_por_partido),
    rebotes_aj = (rebotes_pp / minutos_por_partido) * log1p(minutos_por_partido),
    
    # Distribución
    asistencias_aj = (asistencias_pp / minutos_por_partido) * log1p(minutos_por_partido),
    
    # Defensa activa
    robos_aj = (robos_pp / minutos_por_partido) * log1p(minutos_por_partido),
    tapones_aj = (tapones_pp / minutos_por_partido) * log1p(minutos_por_partido),
    
    # Errores y faltas
    perdidas_aj = (perdidas_pp / minutos_por_partido) * log1p(minutos_por_partido),
    faltas_aj = (faltas_pp / minutos_por_partido) * log1p(minutos_por_partido),
    
    # Producción ofensiva total
    puntos_aj = (puntos_pp / minutos_por_partido) * log1p(minutos_por_partido)
  )%>% 
  
   select(jugador,
         edad,
         posicion,
         all_stars, partidos_jugados,
         partidos_titular,
         ends_with("_aj"))

```

Posteriormente, se ha hecho una limpieza de los datos (Ver rmarkdown en *ANEXO 1*.)Se observa que los jugadores anómalos suelen tener muchos partidos y es muy frecuente que sean jugadores All-Stars, lo que indica que no son errores de datos sino jugadores con un rendimiento excepcional.
Se decide no eliminarlos, sin embargo, a los jugadores anómalos que han jugado menos de 8 partidos se ha decidido eliminarlos, por no ser una cifra muy alta para poder obtener conclusiones de esos jugadores.

Tras la limpieza, nuestra base de datos tiene 508 observaciones(jugadores) y 23 variables generalmente numéricas ya que estamos hablando de estadísticas, aun que hay otras categóricas como equipo y all-stars.

# Análisis PCA

## Aplicación del método

El objetivo de realizar un PCA a los jugadores de NBA es identificar patrones de rendimiento y eficiencia entre los jugadores de la NBA, permitiendo descubrir perfiles de juego diferenciados a partir de sus estadísticas individuales.
Mediante la reducción de la dimensionalidad del conjunto de variables, se pretende representar de forma visual y simplificada las principales características que definen a los jugadores, facilitando la detección de estilos de juego, fortalezas y debilidades, así como la comparación objetiva entre ellos.
Se recuerda que las variables han sido normalizadas por minuto y aplicado un escalado logarítmico ponderado por minutos jugados, de modo que ni los jugadores muy productivos con pocos minutos ni los habituales titulares inflen indebidamente el análisis.
Gracias a esta transformación, el PCA representa visualmente las fortalezas, debilidades y perfiles diferenciados de cada jugador de manera más equilibrada y comparable.

```{r Lectura de datos}
# 1. Cargar el dataset

load("trabajo.RData", verbose = TRUE)

nba_ajustado <- read.csv("nba_2022_2023_ajustado_limpio.csv", as.is = TRUE)
head(nba_ajustado)

nba <- read.csv("nba_2022_2023.csv", as.is = TRUE)
```

```{r tipos de datos}
# 2. Construir la tabla de descripción
categ_vars <- c("all_stars") 

descNBA <- data.frame(
  Variable     = colnames(nba_ajustado),
  Tipo_de_Dato = ifelse(
                   colnames(nba_ajustado) %in% categ_vars,
                   "categorica",
                   sapply(nba_ajustado, class)
                 ),
  stringsAsFactors = FALSE
)
print(descNBA)

```

Para poder realizar el PCA, es importante centrar y escalar los datos.
Esto se hace para que todas las variables tengan la misma importancia en el análisis.

```{r Centrado y escalado}
# 3. Centrar y escalar los datos
vars_excluir <- c( "edad", "all_stars", 
                  "partidos_jugados", "partidos_titular")

# Seleccionar las variables numéricas a transformar (sin las excluidas)
vars_num <- nba_ajustado %>%
  select(where(is.numeric)) %>%
  select(-all_of(vars_excluir)) %>%
  colnames()

# Centrar y escalar solo las variables seleccionadas
nba_z <- nba_ajustado
nba_z[vars_num] <- scale(nba_ajustado[vars_num])

# Ver los resultados
head(nba_z)

```

## Selección de variables

En cuanto a las variables que son interesantes para realizar el PCA, se observa que la variable rebotes no se puede usar porque es combinación lineal de las variables "ro" (rebotes ofensivos) y "rd" (rebotes defensivos).
NO interesan las variables partidos_jugados ni partidos_titular, ya que se ha transformado las variables por minuto real y ponderado por minutos jugados por partido, si se incluyen, se corre el riesgo de que la primera componente principal distinga "titulares" de "suplentes" y no el rendimiento de los jugadores que es lo que se busca con el estudio.

Además, la variable all_stars no se usará porque es categórica, edad tampoco es interesante ni el nombre porque es identificativo.
A partir de la matriz de correlaciones, se superime todas las métricas de tiros y anotaciones que están muy ligadas a “puntos_aj” (se ha eliminado tiros_anotados_aj y tiros_intentados_aj,tiros_2_anotados_aj y tiroa_2_intentados_aj ), de modo que la primera componente no acabe midiendo únicamente volumen de anotación.
Tampoco se conserva “triples_anotados_aj” —al compartir alta correlación con “triples_intentados_aj”, preferimos quedarnos con el volumen de intentos como mejor indicador de rol ofensivo— ni “tl_anotados” —pues “tl_intentados_aj” ya refleja la agresividad al forzar faltas.

De esta manera solo se mantienen variables que aportan información específica sobre estilos de juego (intentos de triples y libres), defensa y creación de juego, lo que facilita identificar perfiles de rendimiento y distinguir roles en la pista.(Ver matriz de correlaciones en el *ANEXO 2*.)

Se genera el modelo PCA para todas las posibles componentes principales (o un elevado número de ellas) y se selecciona el número “óptimo” de componentes principales (PCs).

```{r Preparación dataset PCA}
# 1. Crear el dataframe definitivo que se usará en el PCA
nba_pca <- nba_z[, c(
  vars_num_final,                # Variables activas para el PCA
  "posicion", "all_stars",       # Variables cualitativas suplementarias
  "partidos_jugados", "partidos_titular", "jugador"  # Variables cuantitativas suplementarias
)]


# 2. Visualizamos la estructura de la tabla final antes de aplicar el PCA
str(nba_pca)

```

```{r PCA2}
#3. Realizamos PCA
pca_res <- PCA(
  nba_pca, 
  scale.unit = FALSE,  # Ya está escalado
  quali.sup = which(colnames(nba_pca) %in% c("posicion", "all_stars", "jugador")),
  quanti.sup = which(colnames(nba_pca) %in% c("partidos_jugados", "partidos_titular")),
  graph = FALSE
)
# 4. Calcular los valores propios (varianza explicada)
eig.val <- get_eigenvalue(pca_res)

# 5. Calcular la varianza explicada media (para trazar la línea de referencia)
VPmedio <- 100 * (1 / nrow(eig.val))

# 6. Graficar la varianza explicada con la línea de la media
fviz_eig(pca_res, addlabels = TRUE) + 
  geom_hline(yintercept = VPmedio, linetype = 2, color = "red")

kable(eig.val[1:7,])


```

Aplicando tanto la regla del codo como la regla de la media (Kaiser), se concluye que es adecuado conservar dos componentes principales.
La regla del codo muestra un cambio brusco en la pendiente tras la segunda componente, indicando que a partir de ahí la ganancia en varianza explicada es marginal.
Por su parte, la regla de la media establece un umbral del 10%, superado claramente solo por las dos primeras componentes (36.3% y 31.9%), mientras que la tercera queda por debajo.
Por tanto, conservar dos componentes permite simplificar la representación de los datos manteniendo un elevado porcentaje de varianza explicada (68.2%) y facilitando una interpretación visual clara.

```{r PCA}
K = 2
pca_res <- PCA(
  nba_pca, 
  scale.unit = FALSE,  # Ya está escalado
  quali.sup = which(colnames(nba_pca) %in% c("posicion", "all_stars", "jugador")),
  quanti.sup = which(colnames(nba_pca) %in% c("partidos_jugados", "partidos_titular")),
  graph = FALSE, 
  ncp = K
)

```

## Detección de anómalos

El estadístico T2 de Hotelling permite identificar valores anómalos extremos, que podrían estar condicionando el modelo, es decir, la creación de las PCs.

```{r SCORES}
# 1. Crear un factor para marcar observaciones normales y anómalas
anomalia_factor <- factor(miT2 > F95, labels = c("Normal", "Anómalo"))

# 2. Primer gráfico: Componentes 1 vs 2
p1 <- fviz_pca_ind(
  pca_res, 
  axes = c(1, 2), 
  geom = "point", 
  habillage = anomalia_factor, 
  palette = c("grey", "red"),
  addEllipses = FALSE,
  legend.title = "Tipo"
) + 
  ggtitle("Componentes 1 vs 2") + 
  theme_minimal()
# 3. Mostrar los dos gráficos en una sola fila
p1
```

```{r Anómalos T2}

# Ordenar por mayor valor de T² (miT2)
anomalos_df <- nba_pca[anomalas, c("jugador", "posicion", "all_stars", "partidos_jugados", "partidos_titular")]

# Añadir la columna con la puntuación T²
anomalos_df$T2 <- miT2[anomalas]

# Ordenar de mayor a menor T²
anomalos_df <- anomalos_df[order(-anomalos_df$T2), ]

# Ver el resultado
head(anomalos_df)
```

Al fijar el límite de T² al 95 %, se espera por azar que unos 25 de los 508 jugadores lo superen, pero en realidad se detecta nueve más de lo previsto, lo que sugiere la presencia de perfiles verdaderamente extremos.
Sin embargo, muchos de esos “anómalos” han disputado numerosos partidos y varios son All-Stars, lo que apunta a que no se trata de datos erróneos sino de jugadores con roles titulares y de gran impacto, por lo que merece la pena analizarlos en profundidad(Ver en *ANEXO 12*).

```{r}
contribT2 = function (X, scores, loadings, eigenval, observ, cutoff = 2) {
  # X is data matrix and must be centered (or centered and scaled if data were scaled)
  misScoresNorm = t(t(scores**2) / eigenval)
  misContrib = NULL
  for (oo in observ) {
    print(rownames(scores)[oo])
    print(scores[oo,])
    misPCs = which(as.numeric(misScoresNorm[oo,]) > cutoff)
    lacontri = sapply(misPCs, function (cc) (scores[oo,cc]/eigenval[cc])*loadings[,cc]*X[oo,])
    lacontri = rowSums((1*(sign(lacontri) == 1))*lacontri)
    misContrib = cbind(misContrib, lacontri)
  }
  colnames(misContrib) = rownames(misScoresNorm[observ,])
  return(misContrib)
}
```

Los jugadores con mayor T² son Giannis Antetokounmpo, Joel Embiid, Luka Doncic, Trae Young, Damian Lillard y Walker Kessler(Ver en *ANEXO 3*)

Lillard destaca por un rol ofensivo dominante (puntaje elevado, triples y tiros libres intentados, asistencias); Giannis muestra su control de la pintura (puntos, rebotes ofensivos y defensivos) y alto volumen de pérdidas; Embiid concentra desequilibrios en anotación interior (tiros libres, puntos, rebotes defensivos) y pérdidas; Trae Young exhibe picos en creación y definición de juego (asistencias, puntos, tiros libres) junto con pérdidas; y Walker Kessler representa el extremo opuesto como especialista defensivo de élite (tapones y rebotes muy altos, ofensiva reducida).

La SCR mide cuánto se desvían las estadísticas reales de los jugadores respecto a lo que el PCA puede reconstruir.
La mayoría queda bien explicada (SCR baja), pero 10 jugadores superan el umbral al 95 % y 5 de ellos incluso el 99 %, cifras acordes con la probabilidad teórica esperada.
Estos casos indican estilos de juego atípicos o métricas que no encajan bien en las dimensiones principales del modelo.(Ver en *ANEXO 11*)

Ben Simmons destaca por un desajuste en asistencias y un peso secundario de los tiros; Brook Lopez y Jaren Jackson Jr. flaquean en tapones; Jimmy Butler muestra errores de modelo en libres y triples (con contribuciones adicionales de pérdidas y faltas); y Ryan Rollins, novato sin minutos de titular, queda señalado casi exclusivamente por pérdidas.
Además, mientras los cuatro primeros son habituales titulares de la All-Star, Rollins apenas jugó y nunca arrancó de inicio, lo que explica su carácter atípico.(VER EN *ANEXO 4*)

## Interpretación de los resultados del PCA

Las variables han sido coloreadas según su contribución a las dos primeras componentes principales del análisis PCA.
En color azul se representan las variables auxiliares (partidos_jugados y partidos_titular), que no han sido utilizadas directamente en la obtención del modelo, pero se han proyectado sobre el nuevo espacio de componentes para facilitar su interpretación(Ver en *ANEXO 5*).

En el biplot de variables se observa dos ejes principales que capturan los estilos de juego en la NBA.
En primer lugar, la Dimensión Ofensiva (PC1, 36,3 % de la varianza) agrupa de forma muy clara las métricas de volumen de ataque —puntos_aj, asistencias_aj y tl_intentados_aj— junto a la tasa de pérdidas_aj y las variables auxiliares partidos_jugados y partidos_titular.
Esto indica que los jugadores que más minutos disputan y más participan en la anotación también asumen un mayor riesgo de pérdida de balón, algo lógico en quienes tienen un rol ofensivo dominante.

Por otro lado, la Dimensión Defensiva (PC2, 31,9 % de la varianza) está definida por rebotes ofensivos y defensivos (ro_aj, rd_aj), tapones_aj y faltas_aj, lo que refleja que quienes más luchan bajo los tableros y bloquean tiros suelen implicarse también en más infracciones.
Al proyectar estas variables, constatamos que el análisis PCA nos permite diferenciar claramente entre perfiles ofensivos (Jugadores de alto volumen y creación de juego) y defensivos (Especialistas en rebote y protección del aro), ofreciéndonos una representación visual y simplificada de los extremos del rendimiento en la competición(Ver en *ANEXO 6*)

```{r Biplot}
fviz_pca_biplot(
  pca_res,
  axes = c(1, 2),
  label = "var",                  
  repel = TRUE,                   
  col.var = "black",              # Variables en negro (sin gradiente de contribución)
  select.var = list(name = c(      # Eliminar variables no deseadas
    "tripes_intentados_aj", "tl_intentados_aj", "ro_aj", "rd_aj", 
    "asistencias_aj", "tapones_aj", "perdidas_aj", "faltas_aj", "puntos_aj"
  )),
  col.ind = nba_pca$posicion,     # Colorear por posición
  palette = "jco",
  addEllipses = FALSE,             
  legend.title = "Posición",      
  labelsize = 4, 
  pointsize = 2.5
)

save(nba_pca, file = "nba_pca.RData")

```

Dimensión Ofensiva (PC1 - 36,3% de la varianza): - Las variables puntos_aj, tl_intentados_aj, perdidas_aj y en menor medida asistencias_aj tienen vectores largos y apuntan en la misma dirección, indicando que los jugadores con valores altos en esta dimensión son los que más protagonismo tienen en ataque.
- Este grupo incluye a jugadores de posiciones ofensivas como PG (Base) y SG (Escolta), que tienden a generar juego y acumular puntos, pero también a cometer más pérdidas debido a su alta participación en las jugadas.

Dimensión Defensiva (PC2 - 31,9% de la varianza): - Las variables ro_aj (rebotes ofensivos), rd_aj (rebotes defensivos), tapones_aj y faltas_aj están relacionadas con la actividad defensiva y de lucha por el balón.
- Las posiciones de C (Pivot) y PF (Ala-Pivot) dominan en este plano, evidenciando su mayor implicación en acciones defensivas, rebote y protección del aro.

Las posiciones SF (Aleros) y jugadores polivalentes (SG-PG, SF-PF, etc.) aparecen más dispersos, reflejando su versatilidad tanto en defensa como en ataque.

# Clustering

## Aplicación del método

El objetivo de realizar clustering es el de ayudar a un General Manager de la NBA quiere incorporar a un jugador que sea pieza clave de su equipo, pero bien por que el jugador cobra mucho dinero en la agencia libre o bien porque no han conseguido lograr un traspaso con su equipo no lo pueden traer. Debe tener otras alternativas que no desvaríen mucho de sus pensamientos. Esta estrategia de ponderación se alinea con el rol de General Manager, minimizar el riesgo de incorporar perfiles rookies sin recorrido y asegurar que el candidato seleccionado aporte tanto en producción estadística como en veteranía y liderazgo desde el primer momento.

```{r}
rownames(nba_pca) <- nba_pca$jugador
nba_pca$Jugador  <- NULL

# Seleccionar solo variables numéricas para clustering
df_num <- nba_pca %>% select(where(is.numeric))
```

Se utilizará una medida de distancia basada en la distancia de Manhattan, ya que al sumar diferencialmente las discrepancias absolutas en cada estadística (puntos, asistencias, rebotes, experiencia, titularidades), nos ofrece un coste de sustitución lineal y completamente desglosable.
Por ejemplo, si dos jugadores difieren en 2 asistencias y 3 rebotes, su distancia aumentará exactamente en 5 unidades.
De esta forma la agrupación que se haga nos beneficia al buscar jugadores que tengan un estilo de juego muy similar.

```{r}
# 1) Calcula la matriz de distancias Manhattan
midist <- dist(df_num, method = "manhattan")

# 2) Visualiza la matriz de distancias
fviz_dist(
  midist,
  show_labels = TRUE,
  lab_size    = 0.3,
  gradient    = list(low = "#00AFBB", mid = "white", high = "#FC4E07")
) +
  labs(title = "Distancia Manhattan entre jugadores")
```

El mapa de color muestra que los jugadores se agrupan es diversos grupos, que es lo que buscamos.

En primer lugar, se aplicará modelos jerárquicos, utilizando el método de la media que es el que utilizaremos, pero también hemos realizado Ward y K-medoides(Ver *Anexo 7* y *Anexo 8*).
Se empieza estimando el número óptimo de clusters:

## Resultados numéricos y gráficos

```{r kmeans-validation, fig.width=7, fig.height=4, warning=FALSE}
# K-means — Silhouette y WSS
p3 <- fviz_nbclust(
  x          = df_num,
  FUNcluster = kmeans,
  method     = "silhouette",
  k.max      = 10,
  verbose    = FALSE
) + labs(title = "K-means (silhouette)")

p4 <- fviz_nbclust(
  x          = df_num,
  FUNcluster = kmeans,
  method     = "wss",
  k.max      = 10,
  verbose    = FALSE
) + labs(title = "K-means (WSS)")

grid.arrange(p3, p4, ncol = 2)

```

```{r silhouette-comparison, fig.width=9, fig.height=4, warning=FALSE, message=FALSE}
library(cluster)
library(ggsci)   # para pal_npg

# --- 1) Calcula tres clusterings  ---
k <- 10

# Ward jerárquico
clust1  <- hclust(midist, method = "ward.D2")
grupos1 <- cutree(clust1, k = k)

# K-means sobre df_num
set.seed(42)
km_res  <- kmeans(df_num, centers = k, nstart = 25)

# PAM (K-medoides) sobre df_num
pam_res <- pam(df_num, k = k)

# --- 2) Define paletas de color ---
colores_wk <- pal_npg("nrc")(k)
colores_km <- pal_npg("nrc")(k)
colores_pm <- pal_npg("nrc")(k)

# --- 3) Grafica las siluetas en 1×3 ---
par(mfrow = c(1, 3), mar = c(4, 4, 3, 1))

# 1) Silueta Ward
plot(
  silhouette(grupos1, midist),
  col    = colores_wk,
  border = NA,
  main   = paste0("WARD (k=", k,")")
)

# 2) Silueta K-means
plot(
  silhouette(km_res$cluster, midist),
  col    = colores_km,
  border = NA,
  main   = paste0("K-MEANS (k=", k,")")
)

# 3) Silueta PAM
plot(
  silhouette(pam_res$clustering, midist),
  col    = colores_pm,
  border = NA,
  main   = paste0("PAM (k=", k,")")
)

```

Se observa como utilizando K-medias vemos como cada individuo está perfectamente clasificado en su cluster menos alguno que otro.
Por ello se utilizará el método de K-means a la hora de extraer nuestros clusters.

Por otro lado, se ve como el número optimo de clusters que nos salían con los 3 métodos era 2.
En este caso este no serviría de nada, ya que el objetivo es clasificar jugadores por estilo de juego similar que tengan además de experiencia, partidos jugados, etc.
Lo más importante sería que hubieran cuantos más grupos mejor para poder centrarnos en roles de jugadores específicos.(*ANEXO 7 Y ANEXO 8*) Lo que sería bueno para nosotros ya que si seleccionaremos 2 o 3 clusters agruparíamos nuestros 500 jugadores en 3 grupos, lo que no tendría sentido en nuestro objetivo.
Por lo que nos vamos a decantar por seleccionar un total de 10 clusters para que hayan grandes discrepancias entre jugadores, que es lo que se busca.
Por lo que, si quisiéramos ver un jugador alternativo a otro, tendríamos que ir al cluster de ese jugador y ver que otros jugadores hay en el cluster que juegen en su misma posición.
Ya que si por ejemplo, nuestro jugador fuera un base y escogiéramos un pivot no tendría sentido la elección.

## Disusión de los resultados del clustering

```{r kmeans-table}
set.seed(42)
# Ejecuta K-medias con 10 clústeres sobre variables numéricas
km_res15 <- kmeans(df_num, centers = 10, nstart = 25)

# Muestra la tabla de tamaños de cada clúster
table(km_res15$cluster)
```

```{r}
# 1) Asigna el cluster a tu tabla completa
nba_pca$cluster_km <- factor(km_res15$cluster)

# 2) Tabla de jugadores por clúster y posición
tabla_cluster <- nba_pca %>%
  select(Jugador = jugador, Posición = posicion, Cluster = cluster_km) %>%
  arrange(Cluster, Posición, Jugador)

tabla_cluster %>%
  kable(
    caption   = "Jugadores agrupados por clúster y posición",
    row.names = FALSE
  )


# 3) Tabla de perfiles medios de variables numéricas por clúster
tabla_perfiles <- df_num %>%
  mutate(Cluster = km_res15$cluster) %>%
  group_by(Cluster) %>%
  summarise(across(everything(), mean))

tabla_perfiles %>%
  kable(
    caption = "Perfiles medios de cada clúster (K-means)",
    digits  = 2
  )
```

Tras realizar la separación de jugadores en clusters(desde 23 hasta 80 observaciones por cluster) se ha obtenido las siguientes conclusiones de cada uno:

CLUSTER 1- En este grupo se puede apreciar aquellos jugadores que son recientes estrellas de la nba y jugadores que están a poco de convertirse en ellas.
Encontrando jugadores que han sido all-star alguna vez en su carrera o que se han quedado a las puertas de serlo.
Jugadores con gran impacto en cancha para su equipo, que se consideran estrellas de este, normalmente la segunda espada.

CLUSTER 2- Este grupo está formado por aquellos jugadores de rotación de un equipo.
Un equipo suele jugar con uno 12/13 jugadores de media por partido en la temporada regular, y en este grupo se encuentran esos jugadores de banquillo que son de gran importancia para el equipo ya que dan descanso a los jugadores principales, obviamente no son estrellas, pero son necesarios ya que es importante los jugadores de banquillo.

CLUSTER 3- Son aquellos jugadores que tienen su rol especifico en el equipo, que salen y juegan sus 25 minutos desde el banquillo principalmente, pero que si algún partido por diversas situaciones deben jugar de titular lo hacen sin problema cumpliendo a la perfección.
Son jugadores que si se encontraran en otro equipo y tuvieran mayor numero de oportunidades podrían hacerse un hueco en el quinteto inicial sin problema ya que están capacitados para ello.

CLUESTER 4- En este caso se habla de jugadores de rol(rotación de banquillo) de una importancia un tanto menor que en el cluster 2, ya que no contribuyen tanto al juego de su equipo.
Estos no tienen tanto impacto en rebotes ni puntos como en el cluster 2.
Pero podrían pertenecer si su equipo le diera un poco más de importancia en el juego.

CLUSTER 5- Son jugadores que participan en algún que otro partido normalmente cuando se busca dar descanso a alguna parte de la plantilla(en la NBA se juega cada 2 días normalmente y hay mucha sobrecarga).
Estos jugadores normalmente suelen ser veteranos en la liga y gusta mucho a los equipos tenerlos bajo su poder.
Son aquellos jugadores que ya han vivido mucho en la liga y se acostumbran a jugar sus 20-25 partido al año no jugando muchos minutos, pero lo principal es su veteranía a la hora de afrontar las temporadas y ayudar al equipo.

CLUSTER 6- En este cluster tienen presencia jugadores interiores(SF-Alero,PF-AlaPivot y C-Pivot) que tienen un gran peso en su equipo pero son propensos a lesionarse dada su poca participación a lo largo de la temporada.
Estos jugadores son considerados de gran importancia en el quinteto inicial como normalmente tercer mejor jugador, pero cogido con pinzas ya que un jugador pensado para esto debe tener un gran físico y capacidad para estar presente en todos los partidos.
Un GM debe tener mucho cuidado con estos jugadores ya que puede haber un gran desembolso económico en ellos y después que estén gran parte de la temporada lesionados.

CLUSTER 7-Jugadores que no paran quietos en pista.
Siempre están en constante movimiento involucrado en jugadas.
Son aquellos jugadores que se asocian con todo el equipo, que las jugadas pasan por ellos aun sin ser estrellas.
No se quedan en la esquina esperando a que les llegue el triple como un jugador de rotación, sino que a pesar de salir bastante desde el banquillo(aun que también pueden jugar de titular) tienen un rol fundamental para que cuando no están las estrellas el baloncesto de su equipo se juegue de forma fluida.

CLUSTER 8- Son los jugadores que llevan el peso del partido para su equipo aquellos jugadores que pueden anotar 20/25 puntos cada noche sin problema para ayudar a que su equipo consiga la victoria.
Aquellos que siempre tienen la pelota en sus manos controlando el juego de su equipo, unos anotadores natos, pero además son asistidores y grandes defensores que siempre están en contacto con la pelota y que su facilidad para anotar está a años luz con el resto de jugadores.
Aquí sería jugadores diferenciales para su equipo que muchos desearían tener.

CLUSTER 9- En este caso son aquellos jugadores que se consideran de banquillo.
De esos que suelen jugar pero sus minutos son los que normalmente conocidos como de residuos, cuando un partido ya bva perdiendo o ganando de mucho, estos jugadores se encargan de dar descanso a los titulares para evitar posibles lesiones y así ganan experiencia en cancha por si se les necesitara en un momento clave de la temporada. Son jugadores que siempre deben de tener los equipos por cualquier imprevisto que haya, ya que deben cumplir como suplente si se les necesita o si el equipo se encuentra dentro de una mala racha de lesiones.
Por lo que se trata de jugadores que se adaptan perfectamente a su rol y saben lo que es estar muchos partido en el banquillo hasta que les llega su momento para cumplir.

CLUSTER 10- Trata de jugadores que no juegan directamente, o si lo es no es relevante.
Muchos equipos tienen a estos jugadores, que normalmente suelen estar en rodaje con el equipo sobre todo a la hora de realizar los entrenamientos.
Son muy importantes en su rol que es el de que los titulares se sientan incomodos cuando entrenan contra ellos.
Normalmente suelen considerarse jugadores jóvenes que están en desarrollo con un equipo para ver si a base de entrenamientos convence a los entrenadores y consigue hacerse un hueco en el banquillo del equipo.
Son los llamados jugadores de desarrollo.

```{r}
get_sustitutos <- function(nombre) {
  # Extraemos el clúster y la posición del jugador
  info <- nba_pca %>% 
    filter(jugador == nombre) %>%
    select(cluster_km, posicion) %>%
    slice(1)
  
  if (nrow(info) == 0) {
    stop("No existe ese jugador en la base de datos.")
  }
  cl  <- info$cluster_km
  pos <- info$posicion
  
  # Filtramos compañeros de clúster y posición, excluyendo al jugador original
  nba_pca %>%
    filter(cluster_km == cl,
           posicion   == pos,
           jugador    != nombre) %>%
    select(Jugador = jugador,
           Posición = posicion,
           Cluster  = cluster_km)
}


```

Ahora, se hace hincapié en encontrar un jugador que sea similar a T.J.McConnell, este es un jugador bastante importante desde el banquillo, que lleva la producción de su equipo desde esa segunda unidad(banquillo) y que puede realizar sin problemas su rol y ayudar al máximo al equipo.

```{r}
get_sustitutos("T.J. McConnell")
```

Davion Mitchell, Devonte' Graham o Malcolm Brogdon entre otros serían jugadores a por los que ir en lugar de T.J.
McConnell.
Se considera de gran acierto estos jugadores ya que comparten estilos de juego de lo más similares entre ellos, que es lo que buscábamos, todo se consideran esos jugadores de banquillo capaces de generar para sus compañeros y encargados de tener el control de la pelota en aquellos momentos donde los titulares descansan en el partido.

Ha sido una decisión muy acertada seleccionar un gran número de clusters.
Los jugadores proporcionados son muy similares al que se buscaba, por tanto el objetivo que teníamos lo hemos cumplido a la perfección y ya podríamos como GM de una franquicia tomar decisiones muy importantes en un equipo para la creación optima de su plantilla sin desvariar muy de los pensamientos que se tenían en un primer momento.

# Análisis PLS

## Aplicación del método

```{r}
# Para particionar y balancear clases
if (!requireNamespace("caret", quietly = TRUE)) install.packages("caret")
library(caret)

# ------------------------------------------------------------
# 1) LECTURA Y PREPARACIÓN DE DATOS
# ------------------------------------------------------------
nba <- read.csv("nba_pca.csv", stringsAsFactors = FALSE)
names(nba) <- trimws(names(nba))  # limpiar espacios en los nombres

# Convertir all_stars a factor con niveles "No"/"Sí"
nba$all_stars <- factor(nba$all_stars,
                        levels = c(0, 1),
                        labels = c("No", "Sí"))

# Seleccionar solo columnas numéricas (excluyendo all_stars)
vars_num <- nba %>%
  select(-all_stars) %>%
  select(where(is.numeric)) %>%
  names()

X <- nba[, vars_num]
y <- nba$all_stars
```

```{r}
# Conteo absoluto de cada clase en todo el dataset
conteo_total <- table(nba$all_stars)
print(conteo_total)

prop_total <- prop.table(conteo_total)
print(prop_total)
```

Con 480 “No” frente a 28 “Sí” en el conjunto de entrenamiento, la proporción original es casi de 17:1 (unos 5 % de All-Stars).
Si llevásemos esto directamente a 1:1, se tendría que replicar casi 452 jugadores All-Star (de 28 a 480), corriendo el riesgo de sobre ajustar el modelo a muy pocas observaciones genuinas de “Sí”.
Por eso conviene optar por un balance intermedio que aumente la representación de All-Stars sin inflar en exceso la muestra con duplicados idénticos.

Un ratio de 2:1 (“dos No” por cada “Sí”) lleva a apuntar a unos 240 All-Stars en lugar de 28 (porque 480/2 ≈ 240), de modo que se tendría que duplicar 212 casos de “Sí” (240 − 28).
En la práctica, esto crea un total de 720 observaciones (480 No + 240 Sí).
Con este grado de upsampling, nuestro modelo PLS-DA ve un tercio de la muestra como All-Stars, lo cual es suficiente para que aprenda los patrones característicos de esos jugadores sin recurrir a miles de copias idénticas.
En el contexto de la NBA, donde los All-Stars representan una minoría muy reducida pero de gran importancia (queremos detectarlos con alta sensibilidad), un ratio de 2:1 ofrece un buen compromiso: mejora la capacidad para identificar rasgos distintivos de All-Stars sin sacrificar tanta variabilidad como para sobre ajustar.
Si tras esto se observa que todavía demasiado sesgo (por ejemplo, muchos falsos positivos o falsos negativos).

```{r}
# ------------------------------------------------------------
# 2) DIVIDIR ENTRE ENTRENAMIENTO Y TEST
# ------------------------------------------------------------
set.seed(42)
train_idx <- createDataPartition(y, p = 0.7, list = FALSE)

X_train <- X[train_idx, ]
X_test  <- X[-train_idx, ]
y_train <- y[train_idx]
y_test  <- y[-train_idx]
```

```{r UPsampling}
# ------------------------------------------------------------
# 3) UPSAMPLING PARCIAL EN EL CONJUNTO DE ENTRENAMIENTO
# ------------------------------------------------------------
# Queremos una proporción aproximada de 2:1 (dos No por cada Sí)
tabla_orig <- table(y_train)
n_no <- tabla_orig["No"]
n_si <- tabla_orig["Sí"]

# Objetivo: duplicar los Sí hasta que haya aproximadamente n_no / 2
target_si <- round(n_no / 2)
n_to_dup <- max(0, target_si - n_si)

# Índices de los ejemplos "Sí"
idx_si <- which(y_train == "Sí")

# Si no hay suficientes "Sí" para duplicar, repetimos con reemplazo
set.seed(42)
dup_idxs <- sample(idx_si, size = n_to_dup, replace = TRUE)

# Construimos X_train_parcial y y_train_parcial
X_train_parcial <- rbind(X_train, X_train[dup_idxs, ])
y_train_parcial <- factor(c(as.character(y_train), rep("Sí", n_to_dup)),
                          levels = c("No", "Sí"))

# Verificamos proporción final
# table(y_train_parcial)  # Debería mostrar aproximadamente 2:1 (No:Sí)
```

```{r Estimación del modelo y optimización del número de componentes}
# ------------------------------------------------------------
# 4) AJUSTAR PLS-DA SOBRE DATOS BALANCEADOS PARCIALMENTE
# ------------------------------------------------------------
set.seed(42)
myplsda_bal <- opls(
  x         = X_train_parcial,
  y         = y_train_parcial,
  predI     = 10,          # prueba hasta 10 componentes
  crossvalI = 10,          # validación cruzada 10‐fold interna
  scaleC    = "standard",  # escala X automáticamente
  fig.pdfC = "none" 
)

```

```{r}
maxNC = 10
plot(1:maxNC, myplsda_bal@modelDF$`R2Y(cum)`, type = "o", pch = 16, col = "blue3",
     lwd = 2, xlab = "Components", ylab = "", ylim = c(0.4,0.8),
     main = "PLS-DA model: NBA")
lines(1:maxNC, myplsda_bal@modelDF$`Q2(cum)`, type = "o", pch = 16, col = "red3",
      lwd = 2)
abline(h = 0.5, col = "red3", lty = 2)
legend("bottomleft", c("R2Y", "Q2"), lwd = 2, 
       col = c("blue3", "red3"), bty = "n")

```

En este contexto, elegir tres componentes Latentes resulta adecuado porque a partir de la tercera componente la ganancia en R²Y se estabiliza cerca de 0.68–0.70 y el valor de Q² comienza a descender levemente.
Al seleccionar cinco componentes comprobamos que tenemos un alto poder explicativo (R²Y) sin sacrificar significativamente la capacidad predictiva (Q²), evitando así sobre ajustar el modelo mientras capturamos la mayor varianza útil para discriminar All-Stars.

## Resultados numéricos y gráficos

```{r}
set.seed(42)
myplsda_bal <- opls(
  x         = X_train_parcial,
  y         = y_train_parcial,
  predI     = 3,          # prueba hasta 5 componentes
  crossvalI = 10,          # validación cruzada 10‐fold interna
  scaleC    = "standard",  # escala X automáticamente            
)
```

Los cuatro paneles ilustran que, con tres componentes, nuestro PLS‐DA capta eficazmente la variabilidad de la dicotomía All‐Star/no All‐Star sin sobreajuste (R²Y se estabiliza en ≈0,66–0,68 y Q² alcanza ≈0,66), y que este poder discriminante no es fruto del azar (los modelos con etiquetas permutadas quedan muy por debajo de los valores reales, p = 0,05). El diagnóstico de observaciones muestra que casi ningún jugador excede los umbrales de distancia de scores (SD) u ortogonal (OD), lo que indica ausencia de outliers extremos, y el score plot de t₁ vs. t₂ revela una separación clara: los All‐Stars se sitúan en valores altos de t₁ y los no All‐Stars en la parte izquierda del espacio.

```{r}
plot(x = myplsda_bal,
     typeVc       = "x-loading",
     parCompVi     = c(1, 2),
     parPaletteVc  = NA,
     parTitleL     = TRUE,
     parCexMetricN = 0.8)

```

En este diagrama de loadings se aprecia cómo cada estadística de jugador se proyecta sobre las dos primeras componentes latentes: la primera (p1, 42 %) agrupa variables ofensivas fuertes como “tl_intentos_aj”, “puntos” y “perdidas_aj”, lo que indica que los All-Stars suelen tener alto volumen de tiro y anotación (y, por la agresividad ofensiva, más pérdidas), mientras que “asistencias_aj” y “partidos_titular” también cargan positivamente en p1, señalando que estar de titular y repartir asistencias caracterizan ese perfil.
La segunda componente (p2, 7 %) resalta acciones defensivas (“robos_aj”, “tapones_aj”) en valores positivos, diferenciando a los jugadores que, además de anotar, aportan robos y bloqueos.
En conjunto, estos loadings muestran que los All-Stars se distinguen tanto por su impacto ofensivo (p1) como por contribuciones defensivas (p2).

La Componente 1 es la que mejor discrimina la clase(R = 0.7878); quienes obtienen valores altos de t1 se proyectan en un u1 alto (grupo All-Star), y quienes tienen t1 bajo se ubican en u1 bajo (No All-Star).
La Componente 2 aporta información adicional de discriminación (más suavemente, R=0.3614), detectando matices que no quedaron en t1, pero no es tan potente para clasificar por sí sola.Ç La Componente 3 ya no es relevante para separar las clases(R=0.1669); solo matiza pequeñas diferencias residuales que no capturaron las dos primeras componentes(Ver *anexo 9*)

Las variables con VIP \> 1 (“puntos_aj”, “tl_intentos_aj”, “perdidas_aj”, “partidos_titular” y “asistencias_aj”) son claramente las más relevantes para distinguir All-Stars de no All-Stars.
“puntos_aj” (≈ 1.53) y “tl_intentos_aj” (≈ 1.44) reflejan que el volumen de anotación y la agresividad ofensiva—más intentos de tiro generando pérdidas (“perdidas_aj” ≈ 1.22) son rasgos distintivos de un All-Star.
Además, figurar como “partidos_titular” (≈ 1.30) y repartir “asistencias_aj” (≈ 1.05) señalan el rol de líder en la cancha, típico de quienes reciben votos o son vistos como piezas clave en su equipo(*Ver en ANEXO 10*).

## Discusión de los resultados del PLS-DA

```{r}
# ------------------------------------------------------------
# 3.5 Medidas del error en PLS-DA para el conjunto de entrenamiento
# ------------------------------------------------------------

# 1) Generar predicciones sobre el conjunto de entrenamiento balanceado
#    Usamos el modelo 'myplsda_bal' y las mismas variables X_train_parcial
mypred_train <- predict(myplsda_bal, newdata = X_train_parcial)

# 2) Cargar el paquete caret (si no está instalado: install.packages("caret"))
library(caret)

# 3) Calcular la matriz de confusión y métricas de clasificación
#    Indicamos que la clase “Sí” es la positiva (All-Star)
cm_train <- confusionMatrix(mypred_train, y_train_parcial, positive = "Sí")

# 4) Mostrar resultados
print(cm_train)

```

Se observa un buen ajuste en entrenamiento: Con un 93.45 % de accuracy, el modelo aprende a distinguir correctamente en su propio conjunto de entrenamiento tanto All-Stars como No‐All-Stars.
Hay que tener cuidado de sobreajuste: Estos valores provienen del mismo conjunto con el que se ha entrenado (upsampleado).
Para verificar que el modelo no está memorizando patrones del train, hay que corroborar su desempeño sobre el conjunto de test original (sin duplicados).

```{r}
# ------------------------------------------------------------
# 3.5 Medidas del error en PLS-DA para el conjunto de entrenamiento
# ------------------------------------------------------------

# 1) Generar predicciones sobre el conjunto de entrenamiento balanceado
#    Usamos el modelo 'myplsda_bal' y las mismas variables X_train_parcial
mypred_test <- predict(myplsda_bal, newdata = X_test)

# 2) Cargar el paquete caret (si no está instalado: install.packages("caret"))
library(caret)

# 3) Calcular la matriz de confusión y métricas de clasificación
#    Indicamos que la clase “Sí” es la positiva (All-Star)
cm_test <- confusionMatrix(mypred_test, y_test, positive = "Sí")

# 4) Mostrar resultados
print(cm_test)

```

En resumen, en test el modelo no pierde ningún All-Star (sensibilidad perfecta), pero “paga el precio” de incluir algunos no All-Stars como falsos positivos, resultando en una precisión moderada sobre los “Sí” predichos.
Estos falsos positivos se podrían considerar all-stars en el caso en el que el criterio de los entrenadores/periodistas basados en las estadísticas tuviera el 100% del peso en la votacion.
En realidad, esta votacion se realiza al 50% entre entrenadores/periodistas y el voto del público.
Por ello nuestro modelo al tener solamente en cuenta las estadísticas, los jugadores que predecimos sí que podrían ser considerados all star, pero debido a los fans de los equipos y sus jugadores votan por fanatismo y no por meritocracia decantando la balanza por los jugadores más mediticos.

```{r conf_matrices, fig.width = 8, fig.height = 4, dpi = 300}
## ```{r conf_matrices, fig.width = 8, fig.height = 4, dpi = 300}
## ===============================================================

library(ggplot2)
library(dplyr)
library(patchwork)

## 1. Paleta de cuatro colores
##    • Aciertos (No–No / Sí–Sí)  → verde
##    • Errores  (No–Sí / Sí–No)  → amarillo
colores_celda <- c(
  "No_No" = "#4daf4a",   # Verd. No  – Pred. No
  "Sí_Sí" = "#4daf4a",   # Verd. Sí  – Pred. Sí
  "No_Sí" = "#ffd92f",   # Falso positivo
  "Sí_No" = "#ffd92f"    # Falso negativo
)

## ---------------------------------------------------------------
## 2. MATRIZ DE CONFUSIÓN – TRAIN
## ---------------------------------------------------------------
pred_labels_train <- predict(myplsda_bal, newdata = X_train_parcial)

cfm_df_train <- as.data.frame(table(
  Real       = y_train_parcial,
  Predicción = pred_labels_train
)) %>%
  mutate(Celda = paste(Real, Predicción, sep = "_"))

p_train <- ggplot(cfm_df_train, aes(Predicción, Real, fill = Celda)) +
  geom_tile(colour = "white") +
  geom_text(aes(label = Freq), size = 5, colour = "black") +
  scale_fill_manual(values = colores_celda, guide = "none") +
  labs(title = "Train", x = "Predicción", y = "Real") +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(face = "bold", hjust = .5),
    panel.grid = element_blank()
  )

## ---------------------------------------------------------------
## 3. MATRIZ DE CONFUSIÓN – TEST
## ---------------------------------------------------------------
pred_labels_test <- predict(myplsda_bal, newdata = X_test)

cfm_df_test <- as.data.frame(table(
  Real       = y_test,
  Predicción = pred_labels_test
)) %>%
  mutate(Celda = paste(Real, Predicción, sep = "_"))

p_test <- ggplot(cfm_df_test, aes(Predicción, Real, fill = Celda)) +
  geom_tile(colour = "white") +
  geom_text(aes(label = Freq), size = 5, colour = "black") +
  scale_fill_manual(values = colores_celda, guide = "none") +
  labs(title = "Test", x = "Predicción", y = "Real") +
  theme_minimal(base_size = 16) +
  theme(
    plot.title = element_text(face = "bold", hjust = .5),
    panel.grid = element_blank()
  )

## ---------------------------------------------------------------
## 4. MOSTRAR RESULTADOS
##    - Solo train:  p_train
##    - Solo test :  p_test
##    - Ambos     :  p_train + p_test + plot_layout(ncol = 2)
## ---------------------------------------------------------------
p_train + p_test + plot_layout(ncol = 2)

```

Con el upsampling 2:1 logramos un PLS-DA que separa bien a los All-Stars sin quedar sesgado hacia la clase mayoritaria. Con solo dos componentes captamos casi toda la señal: la primera explica el 42 % de R²X y se correlaciona 0,79 con la etiqueta, mientras la segunda aporta la varianza residual necesaria. Los VIP confirman lo que esperábamos: puntos ajustados e intentos de tiro son los rasgos decisivos, y enseguida aparecen pérdidas, partidos como titular y asistencias.
En entrenamiento obtenemos una balanced accuracy del 93 % (92 % de sensibilidad y 94 % de especificidad); en prueba, detectamos todos los All-Stars y mantenemos una especificidad del 95 %. El coste de no dejar escapar ningún All-Star es un aumento de falsos positivos, de modo que el valor predictivo positivo cae al 53 %, pero en conjunto el modelo generaliza bien y refleja con claridad que el volumen ofensivo es el sello principal de la élite NBA.

# Coclusiones
A lo largo de este trabajo hemos encadenado tres piezas analíticas PCA, k-means y PLS-DA que, combinadas, proporcionan una visión profunda y operativa del ecosistema NBA. En primer lugar, el PCA nos permitió reducir más de veinte variables por jugador a dos ejes que concentran casi el 70 % de la varianza: un eje ofensivo, dominado por puntos, intentos de tiro y asistencias, y un eje defensivo, gobernado por rebotes, tapones y faltas. Este espacio latente describe con claridad el continuo ataque/defensa y, gracias a la estandarización por minuto y un ponderado logarítmico de los minutos jugados, elimina el sesgo de uso para comparar con justicia tanto titulares como suplentes productivos. Además, las distancias T² y la SCR revelaron que las observaciones “extremas” corresponden a perfiles reales y valiosos (All-Stars de uso altísimo u especialistas defensivos), por lo que conviene conservarlas para representar los polos del rendimiento.

Sobre ese mismo espacio aplicamos clustering k-means, obteniendo agrupaciones coherentes con la intuición de entrenadores y analistas: bases creadores de tiro exterior, alas anotadores polivalentes, grandes protectores del aro, especialistas de rol, etc. Esta segmentación nos permitió desarrollar la función get_sustitutos(), que identifica candidatos de la misma posición y clúster para reemplazar a un jugador determinado. De este modo aportamos una herramienta práctica de scouting: basta con un nombre para obtener alternativas de perfil similar, útil en rotaciones, traspasos o ajustes por lesión.

Por último, pusimos a prueba la capacidad discriminante de las mismas variables mediante un PLS-DA dirigido a clasificar jugadores en All-Stars y no All-Stars. Con solo tres componentes alcanzamos R²Y ≈ 0,68 y Q² ≈ 0,66. El modelo señala como motores principales de la condición All-Star el volumen ofensivo (puntos y tiros libres) y la creación (asistencias), mientras que las pérdidas y ciertos indicadores defensivos actúan como contrapeso. 


# Anexos

## Anexo 1: <https://github.com/cofrian/NBA_MDP/blob/main/limpieza_tratamiento_datos.Rmd>

## Anexo 2:

```{r Seleccion de variables y limpieza de observaciones para el estudio del PCA}

## --- Paso previo: eliminar variables muy correlacionadas --------------------
vars_eliminar <- c(
  "tiros_anotados_aj", "tiros_intentados_aj",
  "triples_anotados_aj",
  "tiros_2_anotados_aj", "tiros_2_intentados_aj",
  "tl_anotados_aj", "robos_aj"
)

vars_num_final <- setdiff(vars_num, vars_eliminar)   # variables que sí entran al PCA
vars_num_final                                      # comprobación

## --- Paso 6: matriz de correlaciones y corrplot final -----------------------
library(corrplot)

corr_matrix_final <- cor(nba_z[vars_num_final])

# Ajustamos márgenes y tamaños de texto para que todas las cifras quepan
par(mar = c(1, 1, 1, 1))        # márgenes compactos

corrplot(
  corr_matrix_final,
  method        = "color",
  type          = "upper",
  tl.col        = "black",
  tl.cex        = 0.8,          # tamaño de etiquetas de variable
  number.cex    = 0.55,         # tamaño de los coeficientes
  number.digits = 2,            # redondeo a 2 decimales
  number.font   = 2,            # (opcional) negrita
  addCoef.col   = "black",
  col           = colorRampPalette(c("blue", "white", "red"))(200),
  diag          = FALSE
)

```

## Anexo 3:

```{r}
# --- Chunk options (si usas R Markdown / Quarto) ---
# ```{r fig.width=6, fig.height=4, dpi=100}

# 1. Filtrar los jugadores anómalos
jugadores_anom <- c(
  "Giannis Antetokounmpo", "Joel Embiid",
  "Trae Young", "Damian Lillard", "Walker Kessler"
)
observ_anom <- which(nba_pca$jugador %in% jugadores_anom)

# 2. Preparar cargas y matriz X (igual que tú)
misLoadings <- sweep(
  pca_res$var$coord, 2,
  sqrt(pca_res$eig[1:K,1]), FUN = "/"
)
X <- as.matrix(nba_pca[vars_num_final])

# 3. Calcular contribuciones a la T²
mycontrisT2 <- contribT2(
  X        = X,
  scores   = misScores,
  loadings = misLoadings,
  eigenval = eig.val[1:K, 1],
  observ   = observ_anom
)

# 4. Layout y estilo para mini-plots
par(
  mfrow     = c(2, 3),        # 2 filas × 3 columnas
  mar       = c(3, 1.5, 2, 0.5),  # márgenes (b, l, t, r)
  oma       = c(0, 0, 0, 0),     # márgenes exteriores
  cex.names = 0.5,             # tamaño de nombres en barras
  cex.axis  = 0.6,             # tamaño de etiquetas de eje
  cex.main  = 0.7,             # tamaño de títulos
  cex.lab   = 0.6              # tamaño de ylab
)

# 5. Dibujar cada barplot
for (i in seq_along(observ_anom)) {
  jugador_nombre <- nba_pca$jugador[observ_anom[i]]
  
  barplot(
    mycontrisT2[, i],
    names.arg = colnames(X),
    las       = 2,
    main      = jugador_nombre,
    col       = "steelblue",
    ylab      = "Contribución a T²"
  )
}

# 6. (Opcional) Resetear parámetros si necesitas volver al layout por defecto
# par(mfrow = c(1, 1))
# par(mar   = c(5, 4, 4, 2) + 0.1)

# ```

```

## Anexo 4:

```{r SCR}
# 1. Lista fija de jugadores con SCR > 99%
jugadores_scr_99 <- c("Ben Simmons", "Brook Lopez", "Jaren Jackson Jr.", 
                      "Jimmy Butler", "Ryan Rollins")

# 2. Localizamos las observaciones en el dataframe
observ_scr_99 <- which(nba_pca$jugador %in% jugadores_scr_99)

# 3. Ajustes de layout para 2 filas × 3 columnas y textos reducidos
par(
  mfrow     = c(2, 3),         # 2 filas, 3 columnas
  mar       = c(5, 2.5, 3, 1),  # márgenes (abajo, izquierda, arriba, derecha)
  oma       = c(0, 0, 0, 0),
  cex.names = 0.6,              # tamaño de nombres de barras
  cex.axis  = 0.7,              # tamaño de etiquetas de ejes
  cex.main  = 0.8,              # tamaño de títulos
  cex.lab   = 0.7               # tamaño de etiquetas (ylab)
)

# 4. Bucle para dibujar cada barplot
for (j in seq_along(observ_scr_99)) {
  idx <- observ_scr_99[j]
  jugador_nombre <- nba_pca$jugador[idx]
  
  # Calculamos las contribuciones (residuo al cuadrado)
  residuos_jugador <- myE[idx, ]^2
  
  barplot(
    residuos_jugador,
    names.arg = colnames(myE),  # nombres que coinciden con el largo del vector
    las       = 2,
    main      = jugador_nombre,
    col       = "darkorange",
    ylab      = "Contribución a SCR"
  )
}

```

## Anexo 5 Gráfico de loadings y scores:

```{r Interpretación de los resulta}

# Visualización de las variables en los dos primeros componentes principales
fviz_pca_var(
  pca_res, 
  axes = c(1, 2), 
  repel = TRUE,  # Evita que los nombres de las variables se solapen
  col.var = "contrib",  # Colorea según la contribución de las variables
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),  # De menor a mayor contribución
  title = "Representación de Variables (Dim 1 y 2)"
)
# 1. Crear un subconjunto eliminando las variables auxiliares
vars_principales <- setdiff(vars_num_final, c("partidos_jugados", "partidos_titular"))
```

## Anexo 6 Gráfico de loadings y scores:

```{r Interpretación de los resultados PCA}
# 2. Visualizar solo las variables activas que participaron en el PCA
fviz_pca_var(
  pca_res,
  axes = c(1, 2),
  select.var = list(name = vars_principales),  # Solo las variables activas
  repel = TRUE,
  col.var = "contrib",
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  title = "Representación de Variables Activas (Dim 1 y 2)"
)

fviz_pca_ind(
  pca_res, 
  axes = c(1, 2), 
  geom = "point",   # Solo puntos, sin etiquetas
  habillage = "posicion", 
  addEllipses = TRUE, 
  palette = "jco", 
  legend.title = "Posición", 
  pointsize = 2.5
)


```

## Anexo 7: Modelo jerárquico de Ward

```{r ward-validation, fig.width=7, fig.height=4, warning=FALSE}
library(factoextra)
library(gridExtra)
# Ward jerárquico (hcut) — Silhouette y WSS
p1 <- fviz_nbclust(
  x          = df_num,
  FUNcluster = hcut,
  method     = "silhouette",
  hc_method  = "ward.D2",
  k.max      = 10,
  diss       = midist,
  verbose    = FALSE
) + labs(title = "Ward (silhouette)")

p2 <- fviz_nbclust(
  x          = df_num,
  FUNcluster = hcut,
  method     = "wss",
  hc_method  = "ward.D2",
  k.max      = 10,
  diss       = midist,
  verbose    = FALSE
) + labs(title = "Ward (WSS)")

grid.arrange(p1, p2, ncol = 2)
```

## Anexo 8: Modelo jerárquico de Ward

```{r pam-validation, fig.width=7, fig.height=4, warning=FALSE}
# K-medoides (PAM) — Silhouette y WSS
p5 <- fviz_nbclust(
  x          = df_num,
  FUNcluster = pam,
  method     = "silhouette",
  k.max      = 10,
  verbose    = FALSE
) + labs(title = "PAM (silhouette)")

p6 <- fviz_nbclust(
  x          = df_num,
  FUNcluster = pam,
  method     = "wss",
  k.max      = 10,
  verbose    = FALSE
) + labs(title = "PAM (WSS)")

grid.arrange(p5, p6, ncol = 2)

```

## Anexo 9:

```{r}
# ------------------------------------------------------------
# Relación lineal t vs u para las tres primeras componentes
# ------------------------------------------------------------

# Extraer los scores de X (T) y de Y (U) del modelo PLS‐DA
T_mat <- myplsda_bal@scoreMN    # matriz n × A (A ≥ 3)
U_mat <- myplsda_bal@uMN        # matriz n × A

# Calcular las correlaciones t_i vs u_i para i = 1, 2, 3
cors <- sapply(1:3, function(i) cor(T_mat[, i], U_mat[, i]))
names(cors) <- paste0("Comp", 1:3)
print(cors)
# Ejemplo de salida:
#  Comp1  Comp2  Comp3 
#  0.812  0.429  0.187 

# Dibujar un scatter‐plot t_i vs u_i para cada componente (1–3)
par(mfrow = c(1, 3), mar = c(4, 4, 2, 1))

for (i in 1:3) {
  plot(
    T_mat[, i], U_mat[, i],
    xlab = paste0("t (Componente ", i, ")"),
    ylab = paste0("u (Componente ", i, ")"),
    main = paste0("Component ", i, " (r = ", round(cors[i], 3), ")"),
    pch  = 16, col = "red3"
  )
  abline(a = 0, b = 1, col = "grey", lty = 3)
}

# Restaurar parámetros gráficos por defecto (opcional)
par(mfrow = c(1, 1), mar = c(5, 4, 4, 2) + 0.1)

```

## Anexo 10:

```{r}
# VIP scores (vector nombrado)
vip_scores <- myplsda_bal@vipVn
vip_scores

# -----------------------------------------------
# 2) GRAFICAR VIP SCORES COMO BARRAS ORDENADAS
# -----------------------------------------------

library(dplyr)
library(ggplot2)

# Convertir a data.frame y ordenar
vip_df <- data.frame(
  Variable = names(vip_scores),
  VIP      = as.numeric(vip_scores),
  row.names = NULL
) %>%
  arrange(desc(VIP))

# Asegurarse de que el factor respete el orden
vip_df$Variable <- factor(vip_df$Variable, levels = vip_df$Variable)

# Gráfico de barras horizontal con línea de VIP = 1
ggplot(vip_df, aes(x = Variable, y = VIP)) +
  geom_col(fill = "steelblue") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  coord_flip() +
  labs(
    title = "VIP Scores del modelo PLS-DA",
    x = "Variable",
    y = "VIP"
  ) +
  theme_minimal(base_size = 12)
```

## Anexo 11:

```{r SCR2, error=FALSE, warning=FALSE, message=FALSE}
# 0. Parámetros iniciales
K_orig     <- 3                            # componentes deseados
scores_mat <- pca_res$ind$coord            # matriz de scores (n × total_pcs)
vars       <- vars_num_final               # variables activas

# 1. Definir X con las variables realmente presentes en nba_pca
vars_X <- intersect(vars, colnames(nba_pca))
X      <- as.matrix(nba_pca[, vars_X])     # n × p_actual

# 2. Ajustar K al máximo disponible en scores y loadings
maxK_scores   <- if (!is.null(dim(scores_mat))) ncol(scores_mat) else 0
loadings_mat  <- pca_res$var$coord         # matriz de loadings (p_total × total_pcs)
maxK_loadings <- if (!is.null(dim(loadings_mat))) ncol(loadings_mat) else 0
K             <- min(K_orig, maxK_scores, maxK_loadings)

# 3. Filtrar variables para los loadings (deben estar en ambas)
vars_loadings <- intersect(vars_X, rownames(loadings_mat))
if (length(vars_loadings) == 0) {
  stop("No se encontraron variables comunes para los loadings.")
}

# 4. Calcular los loadings 'puros' (p_act × K)
misLoadings <- sweep(
  loadings_mat[vars_loadings, 1:K, drop=FALSE],
  2,
  sqrt(pca_res$eig[1:K, 1]),
  FUN = "/"
)

# 5. Asegurar que X y misLoadings coincidan en columnas
X <- X[, vars_loadings, drop=FALSE]        # n × p_act

# 6. Matriz de residuos (n × p_act)
if (K > 0) {
  myE <- X - scores_mat[, 1:K, drop=FALSE] %*% t(misLoadings)
} else {
  myE <- X * 0  # si K=0, residuos iguales a X
}

# 7. Cálculo de la SCR y umbrales de control
mySCR      <- rowSums(myE^2)
g          <- var(mySCR) / (2 * mean(mySCR))
h          <- (2 * mean(mySCR)^2) / var(mySCR)
chi2lim_95 <- g * qchisq(0.95, df = h)
chi2lim_99 <- g * qchisq(0.99, df = h)

# 8. Graficar la SCR con líneas de umbral
plot(
  seq_along(mySCR), mySCR, type = "l",
  main = "Distancia al Modelo PCA (SCR)",
  xlab = "Índice de Jugador", ylab = "SCR",
  ylim = c(0, max(mySCR, chi2lim_99) * 1.1)
)
abline(h = chi2lim_95, col = "orange", lty = 2, lwd = 2)
abline(h = chi2lim_99, col = "red3",   lty = 2, lwd = 2)

# 9. Mostrar jugadores que superan umbrales
jug95 <- which(mySCR > chi2lim_95)
jug99 <- which(mySCR > chi2lim_99)
cat("Jugadores con SCR > 95%:\n")
print(nba_pca$jugador[jug95])
cat("Jugadores con SCR > 99%:\n")
print(nba_pca$jugador[jug99])

```

## Anexo 12:

```{r T2 DE HOTELLING}

# 1. Extraer las coordenadas (scores) de los jugadores en los K primeros componentes
misScores <- pca_res$ind$coord[, 1:K]

# 3. Calcular la estadística Hotelling's T² para cada jugador
miT2 <- colSums(t(misScores^2) / eig.val[1:K, 1])

# 4. Calcular los umbrales de control (F-distribution thresholds)
I <- nrow(nba_pca)  # Número de jugadores (individuos)

F95 <- K * (I^2 - 1) / (I * (I - K)) * qf(0.95, K, I - K)  # Umbral del 95%
F99 <- K * (I^2 - 1) / (I * (I - K)) * qf(0.99, K, I - K)  # Umbral del 99%

# 5. Gráfico de Hotelling T² para detección de anomalías
plot(
  1:length(miT2), miT2, type = "p", 
  xlab = "Jugadores", 
  ylab = "Estadístico T² de Hotelling",
  main = "Detección de Anomalías con Hotelling T²"
)
abline(h = F95, col = "orange", lty = 2, lwd = 2)  # Umbral 95%
abline(h = F99, col = "red3", lty = 2, lwd = 2)    # Umbral 99%

anomalas = which(miT2 > F95)
anomalas # Jugadores anómalos)
```

## Anexo 13:

```{r Gráfico de dispersión}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor = 5, ...) {
  par(usr = c(0, 1, 0, 1))
  r <- cor(x, y, use = "complete.obs")
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  # Aquí controlas el tamaño de la fuente (ajústalo si quieres aún más grande)
  text(0.5, 0.5, txt, cex = cex.cor * abs(r), font = 2)
}



# Variables ofensivas
vars_ofensivas <- c("puntos_aj", "tl_intentados_aj","asistencias_aj", "perdidas_aj", 
                    "partidos_jugados", "partidos_titular")

pairs(
  nba_pca[, vars_ofensivas],
  lower.panel = panel.cor, 
  pch = 20, 
  col = "red3",
  main = "Matriz de Dispersión - Variables Ofensivas"
)

# Variables defensivas
vars_defensivas <- c("ro_aj", "rd_aj", "tapones_aj", "faltas_aj", "partidos_jugados", "partidos_titular")

pairs(
  nba_pca[, vars_defensivas],
  lower.panel = panel.cor, 
  pch = 20, 
  col = "steelblue",
  main = "Matriz de Dispersión - Variables Defensivas"
)


```
